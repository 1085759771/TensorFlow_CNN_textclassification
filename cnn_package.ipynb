{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import tensorflow.contrib.keras as kr\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "\n",
    "class TextConfig():\n",
    "    vocab_size = 5000\n",
    "    seq_length = 600\n",
    "    embedding_dim = 64  # 词向量维度\n",
    "    num_filters = 256  # 卷积核数目\n",
    "    kernel_size = 5  # 卷积核尺\n",
    "    hidden_dim = 128  # 全连接层神经元\n",
    "    dropout_keep_prob = 0.5  # dropout保留比例\n",
    "    learning_rate = 1e-3  # 学习率\n",
    "    batch_size = 32  # 每批训练大小\n",
    "    num_iteration = 5000 #迭代次数\n",
    "    print_per_batch = num_iteration / 20 #打印间隔\n",
    "\n",
    "class TextClassification():\n",
    "    def config(self):\n",
    "        textConfig = TextConfig()\n",
    "        self.vocab_size = textConfig.vocab_size\n",
    "        self.seq_length = textConfig.seq_length\n",
    "        self.embedding_dim = textConfig.embedding_dim\n",
    "        self.num_filters = textConfig.num_filters\n",
    "        self.kernel_size = textConfig.kernel_size\n",
    "        self.hidden_dim = textConfig.hidden_dim\n",
    "        self.dropout_keep_prob = textConfig.dropout_keep_prob\n",
    "        self.learning_rate = textConfig.learning_rate\n",
    "        self.batch_size = textConfig.batch_size\n",
    "        self.print_per_batch = textConfig.print_per_batch\n",
    "        self.num_iteration = textConfig.num_iteration\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        self.config()\n",
    "        if len(args) == 2:\n",
    "            content_list = args[0]\n",
    "            label_list = args[1]\n",
    "            train_X, test_X, train_y, test_y = train_test_split(content_list, label_list)\n",
    "            self.train_content_list = train_X\n",
    "            self.train_label_list = train_y\n",
    "            self.test_content_list = test_X\n",
    "            self.test_label_list = test_y\n",
    "            self.content_list = self.train_content_list + self.test_content_list\n",
    "        elif len(args) == 4:\n",
    "            self.train_content_list = args[0]\n",
    "            self.train_label_list = args[1]\n",
    "            self.test_content_list = args[2]\n",
    "            self.test_label_list = args[3]\n",
    "            self.content_list = self.train_content_list + self.test_content_list\n",
    "        else:\n",
    "            print('false to init TextClassification object')\n",
    "        self.autoGetNumClasses()\n",
    "    \n",
    "    def autoGetNumClasses(self):\n",
    "        label_list = self.train_label_list + self.test_label_list\n",
    "        self.num_classes = np.unique(label_list).shape[0]\n",
    "    \n",
    "    def getVocabularyList(self, content_list, vocabulary_size):\n",
    "        allContent_str = ''.join(content_list)\n",
    "        counter = Counter(allContent_str)\n",
    "        vocabulary_list = [k[0] for k in counter.most_common(vocabulary_size)]\n",
    "        return ['PAD'] + vocabulary_list\n",
    "\n",
    "    def prepareData(self):\n",
    "        vocabulary_list = self.getVocabularyList(self.content_list, self.vocab_size)\n",
    "        if len(vocabulary_list) < self.vocab_size:\n",
    "            self.vocab_size = len(vocabulary_list)\n",
    "        contentLength_list = [len(k) for k in self.train_content_list]\n",
    "        if max(contentLength_list) < self.seq_length:\n",
    "            self.seq_length = max(contentLength_list)\n",
    "        self.word2id_dict = dict([(b, a) for a, b in enumerate(vocabulary_list)])\n",
    "        self.labelEncoder = LabelEncoder()\n",
    "        self.labelEncoder.fit(self.train_label_list)\n",
    "\n",
    "    def content2idList(self, content):\n",
    "        return [self.word2id_dict[word] for word in content if word in self.word2id_dict]\n",
    "\n",
    "    def content2X(self, content_list):\n",
    "        idlist_list = [self.content2idList(content) for content in content_list]\n",
    "        X = kr.preprocessing.sequence.pad_sequences(idlist_list, self.seq_length)\n",
    "        return X\n",
    "\n",
    "    def label2Y(self, label_list):\n",
    "        y = self.labelEncoder.transform(label_list)\n",
    "        Y = kr.utils.to_categorical(y, self.num_classes)\n",
    "        return Y\n",
    "\n",
    "    def buildModel(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.X_holder = tf.placeholder(tf.int32, [None, self.seq_length])\n",
    "        self.Y_holder = tf.placeholder(tf.float32, [None, self.num_classes])\n",
    "        embedding = tf.get_variable('embedding', [self.vocab_size, self.embedding_dim])\n",
    "        embedding_inputs = tf.nn.embedding_lookup(embedding, self.X_holder)\n",
    "        conv = tf.layers.conv1d(embedding_inputs, self.num_filters, self.kernel_size)\n",
    "        max_pooling = tf.reduce_max(conv, reduction_indices=[1])\n",
    "        full_connect = tf.layers.dense(max_pooling, self.hidden_dim)\n",
    "        full_connect_dropout = tf.contrib.layers.dropout(full_connect, keep_prob=self.dropout_keep_prob)\n",
    "        full_connect_activate = tf.nn.relu(full_connect_dropout)\n",
    "        softmax_before = tf.layers.dense(full_connect_activate, self.num_classes)\n",
    "        self.predict_Y = tf.nn.softmax(softmax_before)\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.Y_holder, logits=softmax_before)\n",
    "        self.loss = tf.reduce_mean(cross_entropy)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train = optimizer.minimize(self.loss)\n",
    "        self.predict_y = tf.argmax(self.predict_Y, 1)\n",
    "        isCorrect = tf.equal(tf.argmax(self.Y_holder, 1), self.predict_y)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(isCorrect, tf.float32))\n",
    "\n",
    "    def trainModel(self):\n",
    "        self.prepareData()\n",
    "        self.buildModel()\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(init)\n",
    "        train_X = self.content2X(self.train_content_list)\n",
    "        train_Y = self.label2Y(self.train_label_list)\n",
    "        test_X = self.content2X(self.test_content_list)\n",
    "        test_Y = self.label2Y(self.test_label_list)\n",
    "        startTime = time.time()\n",
    "        for i in range(self.num_iteration):\n",
    "            selected_index = random.sample(list(range(len(train_Y))), k=self.batch_size)\n",
    "            batch_X = train_X[selected_index]\n",
    "            batch_Y = train_Y[selected_index]\n",
    "            self.session.run(self.train, {self.X_holder: batch_X, self.Y_holder: batch_Y})\n",
    "            step = i + 1\n",
    "            if step % self.print_per_batch == 0 or step == 1:\n",
    "                selected_index = random.sample(list(range(len(test_Y))), k=200)\n",
    "                batch_X = test_X[selected_index]\n",
    "                batch_Y = test_Y[selected_index]\n",
    "                loss_value, accuracy_value = self.session.run([self.loss, self.accuracy],\\\n",
    "                    {self.X_holder: batch_X, self.Y_holder: batch_Y})\n",
    "                used_time = time.time() - startTime\n",
    "                print('step:%d loss:%.4f accuracy:%.4f used time:%.2f seconds' %\n",
    "                      (step, loss_value, accuracy_value, used_time))\n",
    "\n",
    "    def predict(self, content_list):\n",
    "        if type(content_list) == str:\n",
    "            content_list = [content_list]\n",
    "        batch_X = self.content2X(content_list)\n",
    "        predict_y = self.session.run(self.predict_y, {self.X_holder:batch_X})\n",
    "        predict_label_list = self.labelEncoder.inverse_transform(predict_y)\n",
    "        return predict_label_list\n",
    "\n",
    "    def predictAll(self):\n",
    "        predict_label_list = []\n",
    "        batch_size = 100\n",
    "        for i in range(0, len(self.test_content_list), batch_size):\n",
    "            content_list = self.test_content_list[i: i + batch_size]\n",
    "            predict_label = self.predict(content_list)\n",
    "            predict_label_list.extend(predict_label)\n",
    "        return predict_label_list\n",
    "\n",
    "    def printConfusionMatrix(self):\n",
    "        predict_label_list = self.predictAll()\n",
    "        df = pd.DataFrame(confusion_matrix(self.test_label_list, predict_label_list),\n",
    "                     columns=self.labelEncoder.classes_,\n",
    "                     index=self.labelEncoder.classes_)\n",
    "        print('\\n Confusion Matrix:')\n",
    "        print(df)\n",
    "\n",
    "    def printReportTable(self):\n",
    "        predict_label_list = self.predictAll()\n",
    "        reportTable = self.eval_model(self.test_label_list,\n",
    "                                 predict_label_list,\n",
    "                                 self.labelEncoder.classes_)\n",
    "        print('\\n Report Table:')\n",
    "        print(reportTable)\n",
    "        \n",
    "    def eval_model(self, y_true, y_pred, labels):\n",
    "        # 计算每个分类的Precision, Recall, f1, support\n",
    "        p, r, f1, s = precision_recall_fscore_support(y_true, y_pred)\n",
    "        # 计算总体的平均Precision, Recall, f1, support\n",
    "        tot_p = np.average(p, weights=s)\n",
    "        tot_r = np.average(r, weights=s)\n",
    "        tot_f1 = np.average(f1, weights=s)\n",
    "        tot_s = np.sum(s)\n",
    "        res1 = pd.DataFrame({\n",
    "            u'Label': labels,\n",
    "            u'Precision': p,\n",
    "            u'Recall': r,\n",
    "            u'F1': f1,\n",
    "            u'Support': s\n",
    "        })\n",
    "        res2 = pd.DataFrame({\n",
    "            u'Label': ['总体'],\n",
    "            u'Precision': [tot_p],\n",
    "            u'Recall': [tot_r],\n",
    "            u'F1': [tot_f1],\n",
    "            u'Support': [tot_s]\n",
    "        })\n",
    "        res2.index = [999]\n",
    "        res = pd.concat([res1, res2])\n",
    "        return res[['Label', 'Precision', 'Recall', 'F1', 'Support']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('train_content_list.pickle', 'rb') as file:\n",
    "    train_content_list = pickle.load(file)\n",
    "with open('train_label_list.pickle', 'rb') as file:\n",
    "    train_label_list = pickle.load(file)\n",
    "with open('test_content_list.pickle', 'rb') as file:\n",
    "    test_content_list = pickle.load(file)\n",
    "with open('test_label_list.pickle', 'rb') as file:\n",
    "    test_label_list = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "step:1 loss:2.4855 accuracy:0.0650 used time:2.62 seconds\n",
      "step:250 loss:0.7422 accuracy:0.7450 used time:5.35 seconds\n",
      "step:500 loss:0.4297 accuracy:0.8850 used time:8.03 seconds\n",
      "step:750 loss:0.2673 accuracy:0.9450 used time:10.66 seconds\n",
      "step:1000 loss:0.3016 accuracy:0.9100 used time:13.28 seconds\n",
      "step:1250 loss:0.1915 accuracy:0.9500 used time:15.92 seconds\n",
      "step:1500 loss:0.2491 accuracy:0.9400 used time:18.56 seconds\n",
      "step:1750 loss:0.2029 accuracy:0.9450 used time:21.21 seconds\n",
      "step:2000 loss:0.1946 accuracy:0.9450 used time:23.86 seconds\n",
      "step:2250 loss:0.1079 accuracy:0.9800 used time:26.51 seconds\n",
      "step:2500 loss:0.2137 accuracy:0.9300 used time:29.14 seconds\n",
      "step:2750 loss:0.1674 accuracy:0.9450 used time:31.80 seconds\n",
      "step:3000 loss:0.1491 accuracy:0.9450 used time:34.44 seconds\n",
      "step:3250 loss:0.1334 accuracy:0.9700 used time:37.07 seconds\n",
      "step:3500 loss:0.2786 accuracy:0.9450 used time:39.71 seconds\n",
      "step:3750 loss:0.1150 accuracy:0.9700 used time:42.33 seconds\n",
      "step:4000 loss:0.1623 accuracy:0.9600 used time:44.96 seconds\n",
      "step:4250 loss:0.1505 accuracy:0.9650 used time:47.58 seconds\n",
      "step:4500 loss:0.1313 accuracy:0.9700 used time:50.22 seconds\n",
      "step:4750 loss:0.1553 accuracy:0.9650 used time:52.84 seconds\n",
      "step:5000 loss:0.1377 accuracy:0.9550 used time:55.47 seconds\n",
      "\n",
      " Confusion Matrix:\n",
      "      体育   健康   女人   娱乐  房地产   教育   文化   新闻   旅游   汽车   科技   财经\n",
      "体育   985    0    1    5    0    0    3    3    1    0    1    1\n",
      "健康     0  989    2    0    0    0    2    3    2    0    2    0\n",
      "女人     0    8  960    6    0    2   16    2    4    0    2    0\n",
      "娱乐     1    2    0  978    0    0   18    0    1    0    0    0\n",
      "房地产    2    7    2    2  909    0   26   25   11    1    1   14\n",
      "教育     1    1    1    3    1  957    7   15    5    2    3    4\n",
      "文化     1    1    3    8    5    2  962    7    5    5    1    0\n",
      "新闻     2    5    0    3   16    5   60  891    5    1    3    9\n",
      "旅游     1    4    1    5   10    0   22    3  945    3    4    2\n",
      "汽车     2    6    2    2    2    6    3    6    5  959    1    6\n",
      "科技     0    8    0    9    3    0    8   12    3    1  946   10\n",
      "财经     1    6    0    7    8    1   10   14    5    2    4  942\n",
      "\n",
      " Report Table:\n",
      "    Label  Precision   Recall        F1  Support\n",
      "0      体育   0.990891  0.97900  0.984909     1000\n",
      "1      健康   0.956522  0.99000  0.972973     1000\n",
      "2      女人   0.979654  0.96300  0.971256     1000\n",
      "3      娱乐   0.949416  0.97600  0.962525     1000\n",
      "4     房地产   0.957113  0.91500  0.935583     1000\n",
      "5      教育   0.976459  0.95400  0.965099     1000\n",
      "6      文化   0.859712  0.95600  0.905303     1000\n",
      "7      新闻   0.907801  0.89600  0.901862     1000\n",
      "8      旅游   0.939036  0.95500  0.946951     1000\n",
      "9      汽车   0.984631  0.96100  0.972672     1000\n",
      "10     科技   0.979210  0.94200  0.960245     1000\n",
      "11     财经   0.960163  0.94000  0.949975     1000\n",
      "999    总体   0.953384  0.95225  0.952446    12000\n"
     ]
    }
   ],
   "source": [
    "model = TextClassification(train_content_list,\n",
    "                           train_label_list,\n",
    "                           test_content_list,\n",
    "                           test_label_list)\n",
    "model.trainModel()\n",
    "model.printConfusionMatrix()\n",
    "model.printReportTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
